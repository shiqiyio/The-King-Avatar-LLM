import streamlit as st
from ragchat import Model_center
from llm import InternLM

'''
é€šè¿‡ @st.cache_resource è£…é¥°å™¨ï¼Œload_model å‡½æ•°åªä¼šåœ¨é¦–æ¬¡è°ƒç”¨æ—¶æ‰§è¡Œï¼Œç¡®ä¿æ¨¡å‹åªåŠ è½½ä¸€æ¬¡ã€‚éšåæ¯æ¬¡ç”¨æˆ·äº¤äº’æ—¶ï¼Œä»£ç å°†ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹ã€‚
'''
@st.cache_resource
def load_model(model_name_or_path):
    llm = InternLM(model_path=model_name_or_path)
    model_center = Model_center(llm)
    return model_center

# åŠ è½½æ¨¡å‹ï¼Œåªæ‰§è¡Œä¸€æ¬¡
model_name_or_path ='/root/share/new_models/qwen/Qwen2-7B-Instruct'
model_center = load_model(model_name_or_path)

# model_center = Model_center(llm)
if "messages" not in st.session_state:
    st.session_state["messages"] = []     
# åœ¨ä¾§è¾¹æ ä¸­åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªé“¾æ¥
with st.sidebar:
    st.markdown("The-King-Avatar-LLM")
    "[InternLM](https://github.com/InternLM/InternLM.git)"
    "[chat-huyu-ABao](https://github.com/shiqiyio/The-King-Avatar-LLM)"
    "æ„Ÿè°¢[chat-huyu-ABao](https://github.com/hoo01/chat-huyu-ABao.git)"

# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªå‰¯æ ‡é¢˜
st.title("The-King-Avatar-LLM")
st.caption("ğŸš€ A streamlit chatbot powered by InternLM2_7B QLora")
    
# éå†session_stateä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œå¹¶æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸Š
for msg in st.session_state.messages:
    st.chat_message("user").write(msg["user"])
    st.chat_message("assistant").write(msg["assistant"])

# Get user input
if prompt := st.chat_input("æå‡ºä»»ä½•å…³äºã€Šå…¨èŒé«˜æ‰‹ã€‹çš„é—®é¢˜"):
    # Display user input
    st.chat_message("user").write(prompt)
        # ä½¿ç”¨ qa_chain ç”Ÿæˆå›ç­”
    response = model_center.qa_chain_self_answer(prompt)
    
    # å°†é—®ç­”ç»“æœæ·»åŠ åˆ° session_state çš„æ¶ˆæ¯å†å²ä¸­
    st.session_state.messages.append({"user": prompt, "assistant": response}) 
    # æ˜¾ç¤ºå›ç­”
    st.chat_message("assistant").write(response)